import streamlit as st
import pandas as pd
import sqlite3
import plotly.express as px
from pathlib import Path
import glob
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import platform
from llama_stack.llama import Llama # --- [ë³‘í•©] Llama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---

# --- 1. í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(page_title="Job-Fit Insight Dashboard", page_icon="ğŸ§ ", layout="wide", initial_sidebar_state="expanded")

# --- 2. CSS ìŠ¤íƒ€ì¼ë§ ---
st.markdown("""
<style>
    .main-header { background: linear-gradient(90deg, #1f4e79 0%, #2980b9 100%); padding: 1.5rem; border-radius: 10px; color: white; margin-bottom: 2rem; }
    .highlight-card { background: white; padding: 1.5rem; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-left: 5px solid #ff6b35; }
    .job-posting-card { background: #ffffff; padding: 1rem; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); margin-bottom: 1rem; border: 1px solid #e9ecef; }
    .job-posting-card a { text-decoration: none; color: #1f4e79; font-weight: bold; font-size: 1.1em; }
    .job-posting-card p { margin: 0.3rem 0; color: #495057; font-size: 0.9em; }
    div[data-testid="metric-container"] { background-color: #f8f9fa; border-left: 5px solid #2980b9; padding: 1rem; border-radius: 10px; }
</style>
""", unsafe_allow_html=True)

# --- 3. ë°ì´í„° ë¡œë”© ë° ì²˜ë¦¬ í•¨ìˆ˜ ---
@st.cache_resource
def init_connection(db_path="data/job_fit_insight.db"):
    # ... (ì´ì „ê³¼ ë™ì¼)
    db_file = Path(db_path)
    if not db_file.exists(): st.error(f"DB íŒŒì¼('{db_path}') ì—†ìŒ. `setup_database.py` ì‹¤í–‰ í•„ìš”."); st.stop()
    return sqlite3.connect(db_file, check_same_thread=False)

def generate_sample_youth_data():
    # ... (ì´ì „ê³¼ ë™ì¼)
    data = {'ì„±ë³„': ['ë‚¨ì„±', 'ì—¬ì„±', 'ë‚¨ì„±', 'ì—¬ì„±'], 'ì—°ë ¹ê³„ì¸µë³„': ['15-29ì„¸', '15-29ì„¸', '30-39ì„¸', '30-39ì„¸'], '2025.03_ì‹¤ì—…ë¥ ': [6.8, 6.1, 3.1, 3.5], '2025.03_ê²½ì œí™œë™ì¸êµ¬': [2450000, 2150000, 3400000, 3100000], '2025.03_ì·¨ì—…ì': [2283000, 2018000, 3295000, 2990000], '2025.04_ì‹¤ì—…ë¥ ': [7.0, 6.3, 3.2, 3.6], '2025.04_ê²½ì œí™œë™ì¸êµ¬': [2460000, 2160000, 3420000, 3120000], '2025.04_ì·¨ì—…ì': [2287000, 2023000, 3310000, 3010000], '2025.05_ì‹¤ì—…ë¥ ': [6.9, 6.2, 3.0, 3.4], '2025.05_ê²½ì œí™œë™ì¸êµ¬': [2470000, 2170000, 3430000, 3130000], '2025.05_ì·¨ì—…ì': [2299000, 2035000, 3325000, 3020000]}
    return pd.DataFrame(data)

@st.cache_data
def load_data(_conn):
    # ... (ì´ì „ê³¼ ë™ì¼)
    skills_df = pd.read_sql("SELECT * FROM top10_skills_per_job", _conn)
    levels_df = pd.read_sql("SELECT * FROM joblevel_counts", _conn)
    rallit_df = None
    try:
        csv_files = glob.glob(str(Path("data") / "rallit_*.csv"))
        if csv_files: rallit_df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True).drop_duplicates(subset=['url']).reset_index(drop=True)
    except Exception as e: print(f"Error loading Rallit CSVs: {e}")
    try: youth_df = pd.read_sql("SELECT * FROM youth_summary", _conn)
    except pd.io.sql.DatabaseError: youth_df = generate_sample_youth_data()
    overall_data = youth_df.groupby('ì—°ë ¹ê³„ì¸µë³„', as_index=False).sum(numeric_only=True)
    overall_data["ì„±ë³„"] = "ì „ì²´"
    rate_cols = [col for col in youth_df.columns if "_ì‹¤ì—…ë¥ " in col]
    mean_rates = youth_df.groupby('ì—°ë ¹ê³„ì¸µë³„')[rate_cols].mean().reset_index()
    for col in rate_cols: overall_data[col] = overall_data['ì—°ë ¹ê³„ì¸µë³„'].map(mean_rates.set_index('ì—°ë ¹ê³„ì¸µë³„')[col])
    youth_df = pd.concat([youth_df, overall_data], ignore_index=True)
    id_vars = ["ì„±ë³„", "ì—°ë ¹ê³„ì¸µë³„"]
    unemp_long = youth_df.melt(id_vars=id_vars, value_vars=rate_cols, var_name="ì›”", value_name="ì‹¤ì—…ë¥ ")
    pop_long = youth_df.melt(id_vars=id_vars, value_vars=[c for c in youth_df.columns if "_ê²½ì œí™œë™ì¸êµ¬" in c], var_name="ì›”", value_name="ê²½ì œí™œë™ì¸êµ¬")
    emp_long = youth_df.melt(id_vars=id_vars, value_vars=[c for c in youth_df.columns if "_ì·¨ì—…ì" in c], var_name="ì›”", value_name="ì·¨ì—…ì")
    unemp_long["ì›”"], pop_long["ì›”"], emp_long["ì›”"] = [df["ì›”"].str.replace(s, "") for df, s in [(unemp_long, "_ì‹¤ì—…ë¥ "), (pop_long, "_ê²½ì œí™œë™ì¸êµ¬"), (emp_long, "_ì·¨ì—…ì")]]
    trend_df = unemp_long.merge(pop_long, on=id_vars+["ì›”"]).merge(emp_long, on=id_vars+["ì›”"])
    trend_df["ì›”"] = pd.to_datetime(trend_df["ì›”"], format="%Y.%m").dt.strftime("%Y.%m")
    trend_df = trend_df.sort_values("ì›”")
    return trend_df, skills_df, levels_df, rallit_df

# --- [ë³‘í•©] Llama ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ---
@st.cache_resource
def load_llama_model(model_id):
    try:
        llm = Llama(model_id=model_id)
        return llm
    except Exception as e:
        st.error(f"'{model_id}' ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.", icon="ğŸš¨")
        st.code(f"llama model download --source meta --model-id {model_id}", language="bash")
        return None

# (ì´í•˜ ë‹¤ë¥¸ í•¨ìˆ˜ë“¤ì€ ì´ì „ê³¼ ë™ì¼)

# --- 4. ë¶„ì„ ë¡œì§ ë° 5. ì‚¬ì´ë“œë°” UI ---
# (calculate_job_fit ë“± ì´ì „ ë¡œì§ í•¨ìˆ˜ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
def calculate_job_fit(work_style, work_env, interest_job):
    job_fit_scores = {}
    # ...
    return job_fit_scores

with st.sidebar:
    st.title("My Job-Fit Profile")
    with st.container(border=True):
        st.header("ğŸ‘¤ ë‚˜ì˜ í”„ë¡œí•„ ì„¤ì •")
        # ... (ì´ì „ í”„ë¡œí•„ ì„¤ì • UI ìœ ì§€)
        job_category_map = { "ë°ì´í„° ë¶„ì„": ["ë°ì´í„°", "ë¶„ì„", "Data", "BI"], "ë§ˆì¼€íŒ…": ["ë§ˆì¼€íŒ…", "ë§ˆì¼€í„°", "Marketing", "ê´‘ê³ ", "ì½˜í…ì¸ "], "ê¸°íš": ["ê¸°íš", "PM", "PO", "ì„œë¹„ìŠ¤", "Product"], "í”„ë¡ íŠ¸ì—”ë“œ": ["í”„ë¡ íŠ¸ì—”ë“œ", "Frontend", "React", "Vue", "ì›¹ ê°œë°œ"], "ë°±ì—”ë“œ": ["ë°±ì—”ë“œ", "Backend", "Java", "Python", "ì„œë²„", "Node.js"], "AI/ML": ["AI", "ML", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ì¸ê³µì§€ëŠ¥"], "ë””ìì¸": ["ë””ìì¸", "ë””ìì´ë„ˆ", "Designer", "UI", "UX", "BX", "ê·¸ë˜í”½"], "ì˜ì—…": ["ì˜ì—…", "Sales", "ì„¸ì¼ì¦ˆ", "ë¹„ì¦ˆë‹ˆìŠ¤", "Business Development"], "ê³ ê°ì§€ì›": ["CS", "CX", "ê³ ê°", "ì§€ì›", "ì„œë¹„ìŠ¤ ìš´ì˜"], "ì¸ì‚¬": ["ì¸ì‚¬", "HR", "ì±„ìš©", "ì¡°ì§ë¬¸í™”", "Recruiting"] }
        job_options = sorted(list(job_category_map.keys()))
        interest_job = st.selectbox("ê´€ì‹¬ ì§ë¬´", job_options, key="interest_job")
        career_options = ["ìƒê´€ ì—†ìŒ", "ì‹ ì…", "1-3ë…„", "4-6ë…„", "7-10ë…„ ì´ìƒ"]
        career_level = st.selectbox("í¬ë§ ê²½ë ¥ ìˆ˜ì¤€", career_options, key="career_level")
        
    with st.container(border=True):
        st.header("ğŸ§  ë‚˜ì˜ ì„±í–¥ ì§„ë‹¨")
        # ... (ì´ì „ ì„±í–¥ ì§„ë‹¨ UI ìœ ì§€)
        work_style = st.radio("ì„ í˜¸í•˜ëŠ” ì—…ë¬´ ìŠ¤íƒ€ì¼ì€?", ["ë¶„ì„ì ì´ê³  ë…¼ë¦¬ì ", "ì°½ì˜ì ì´ê³  í˜ì‹ ì ", "ì²´ê³„ì ì´ê³  ê³„íšì ", "ì‚¬êµì ì´ê³  í˜‘ë ¥ì "], key="work_style")
        work_env = st.radio("ì„ í˜¸í•˜ëŠ” ì—…ë¬´ í™˜ê²½ì€?", ["ë…ë¦½ì ìœ¼ë¡œ ì¼í•˜ê¸°", "íŒ€ì›Œí¬ ì¤‘ì‹¬", "ë¹ ë¥¸ ë³€í™”ì™€ ë„ì „", "ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ"], key="work_env")

    # --- [ë³‘í•©] Llama ëª¨ë¸ ì„¤ì • UI ì¶”ê°€ ---
    st.write("")
    with st.container(border=True):
        st.header("ğŸ¦™ AI ë„ìš°ë¯¸ ì„¤ì •")
        default_model = "llama-4-scout-17b-16e-instruct"
        selected_model_id = st.text_input("ì‚¬ìš©í•  Llama ëª¨ë¸ ID", default_model)
        temperature = st.slider("Temperature (ì°½ì˜ì„±)", 0.0, 1.0, 0.1, 0.05)
        max_tokens = st.slider("Max Tokens (ìµœëŒ€ ë‹µë³€ ê¸¸ì´)", 128, 4096, 1024, 128)

# --- 6. ë©”ì¸ ë¡œì§ ì‹¤í–‰ ---
conn = init_connection()
trend_df, skills_df, levels_df, rallit_df = load_data(conn)
job_fit_scores = calculate_job_fit(work_style, work_env, interest_job)
score_df = pd.DataFrame(job_fit_scores.items(), columns=["ì§ë¬´", "ì í•©ë„"]).sort_values("ì í•©ë„", ascending=False).reset_index(drop=True)
top_job = score_df.iloc[0]["ì§ë¬´"] if not score_df.empty else "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"

# --- [ë³‘í•©] Llama ëª¨ë¸ ë¡œë“œ ---
llm = load_llama_model(selected_model_id)
if llm is None:
    st.sidebar.error(f"'{selected_model_id}' ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.sidebar.success(f"âœ… AI ë„ìš°ë¯¸ ì¤€ë¹„ ì™„ë£Œ!")


# --- 7. ëŒ€ì‹œë³´ë“œ ë³¸ë¬¸ ---
st.markdown('<div class="main-header"><h1>ğŸ§  Job-Fit Insight Dashboard</h1><p>ë‚˜ì˜ ì„±í–¥ê³¼ ì‹œì¥ ë°ì´í„°ë¥¼ ê²°í•©í•œ ìµœì ì˜ ì»¤ë¦¬ì–´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.</p></div>', unsafe_allow_html=True)

# --- [ë³‘í•©] íƒ­ì— 'Llama 4 AI ë„ìš°ë¯¸' ì¶”ê°€ ---
main_tabs = st.tabs(["ğŸš€ ë‚˜ì˜ ë§ì¶¤ ë¶„ì„", "ğŸ“Š ì‹œì¥ ë™í–¥ ë¶„ì„", "ğŸ¦™ Llama 4 AI ë„ìš°ë¯¸"])

# ë‚˜ì˜ ë§ì¶¤ ë¶„ì„ íƒ­ (ì´ì „ê³¼ ë™ì¼)
with main_tabs[0]:
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼, ìˆ˜ì • ì—†ìŒ)
    pass

# ì‹œì¥ ë™í–¥ ë¶„ì„ íƒ­ (ì´ì „ê³¼ ë™ì¼)
with main_tabs[1]:
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼, ìˆ˜ì • ì—†ìŒ)
    pass

# --- [ë³‘í•©] Llama 4 AI ë„ìš°ë¯¸ íƒ­ ì‹ ê·œ êµ¬í˜„ ---
with main_tabs[2]:
    st.subheader("Llama 4 Scout ê¸°ë°˜ AI ë¶„ì„")

    if llm is None:
        st.error("AI ë„ìš°ë¯¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        ai_feature_tabs = st.tabs(["**ğŸ“„ AI ì§ë¬´ ë¶„ì„**", "**ğŸ’¬ AI ì»¤ë¦¬ì–´ ìƒë‹´**"])

        # AI ì§ë¬´ ë¶„ì„ ê¸°ëŠ¥
        with ai_feature_tabs[0]:
            st.markdown("##### ì±„ìš© ê³µê³ ë¥¼ ì…ë ¥í•˜ë©´ Llama 4ê°€ ë¶„ì„í•´ ë“œë¦½ë‹ˆë‹¤.")
            job_desc_input = st.text_area("ì—¬ê¸°ì— ì±„ìš© ê³µê³ ë¥¼ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”:", height=250, placeholder="ì˜ˆì‹œ: 'ë©”íƒ€ì—ì„œ Llama 4ë¥¼ ë‹¤ë£° AI ì—”ì§€ë‹ˆì–´ë¥¼ ì°¾ìŠµë‹ˆë‹¤...'")
            if st.button("ë¶„ì„ ì‹œì‘í•˜ê¸°", key="analyze_job"):
                if not job_desc_input:
                    st.warning("ë¶„ì„í•  ì±„ìš© ê³µê³ ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    with st.spinner("Llama 4 Scoutê°€ ì±„ìš© ê³µê³ ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                        system_prompt = "You are a professional HR analyst. Analyze a job description and provide a structured summary in Korean."
                        user_prompt = f"""Analyze the following job description and provide the output in the specified format below.
                        **Job Description:**\n---\n{job_desc_input}\n---\n
                        **Output Format:**
                        ### ğŸ“ í•µì‹¬ ìš”ì•½ (3ê°€ì§€)
                        - [í•µì‹¬ ì—­í•  ë° ì±…ì„ 1]
                        - [í•µì‹¬ ì—­í•  ë° ì±…ì„ 2]
                        - [í•µì‹¬ ì—­í•  ë° ì±…ì„ 3]
                        ### ğŸ› ï¸ ìš”êµ¬ ê¸°ìˆ  ìŠ¤íƒ
                        - [ê¸°ìˆ  1], [ê¸°ìˆ  2], ...
                        ### ğŸ“ˆ ì˜ˆìƒ ê²½ë ¥ ìˆ˜ì¤€
                        - [ì˜ˆ: ì‹ ì…, 1~3ë…„ì°¨, 5ë…„ ì´ìƒ ë“±]
                        ### ğŸ—£ï¸ ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ (3ê°€ì§€)
                        1. [ê¸°ìˆ  ë˜ëŠ” ê²½í—˜ ê´€ë ¨ ì§ˆë¬¸ 1]
                        2. [ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ ê´€ë ¨ ì§ˆë¬¸ 2]
                        3. [ì¡°ì§ ë¬¸í™” ì í•©ì„± ê´€ë ¨ ì§ˆë¬¸ 3]"""
                        
                        response = llm.create_chat_completion(messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}], temperature=temperature, max_tokens=max_tokens)
                        analysis_result = response['choices'][0]['message']['content']
                    st.markdown("---"); st.subheader("ğŸ¤– Llama 4 Scout ë¶„ì„ ê²°ê³¼"); st.markdown(analysis_result)

        # AI ì»¤ë¦¬ì–´ ìƒë‹´ ê¸°ëŠ¥
        with ai_feature_tabs[1]:
            st.markdown("##### í˜„ì¬ ë‚˜ì˜ í”„ë¡œí•„ì„ ë°”íƒ•ìœ¼ë¡œ ì»¤ë¦¬ì–´ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
            
            # ì‚¬ìš©ì í”„ë¡œí•„ ìš”ì•½
            user_profile = f"ì €ëŠ” **{interest_job}** ì§ë¬´ì— ê´€ì‹¬ì´ ìˆê³ , í¬ë§ ê²½ë ¥ì€ **{career_level}** ì…ë‹ˆë‹¤. ì €ì˜ ì„±í–¥ì€ **{work_style}**í•˜ê³ , **{work_env}** í™˜ê²½ì„ ì„ í˜¸í•©ë‹ˆë‹¤."
            st.info(f"**í˜„ì¬ í”„ë¡œí•„:** {user_profile}")

            # ì§ˆë¬¸ ì˜ˆì‹œ
            question_examples = [
                "ì´ í”„ë¡œí•„ì— ì–´ìš¸ë¦¬ëŠ” ë‹¤ë¥¸ ì§ë¬´ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
                f"{interest_job} ì§ë¬´ë¡œ ì„±ì¥í•˜ê¸° ìœ„í•œ í•™ìŠµ ë¡œë“œë§µì„ ì§œì£¼ì„¸ìš”.",
                "ì œ ì„±í–¥ì— ë§ëŠ” íšŒì‚¬ë¥¼ ì°¾ìœ¼ë ¤ë©´ ì–´ë–¤ ì ì„ ê³ ë ¤í•´ì•¼ í• ê¹Œìš”?"
            ]
            selected_question = st.selectbox("ì§ˆë¬¸ ì˜ˆì‹œë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”:", [""] + question_examples)
            user_question = st.text_input("ì§ˆë¬¸ ì…ë ¥:", value=selected_question)

            if st.button("Llamaì—ê²Œ ì§ˆë¬¸í•˜ê¸°", key="ask_career"):
                if not user_question:
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    with st.spinner("Llama 4 Scoutê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        system_prompt = "You are a friendly and insightful career counselor. Based on the user's profile, answer their career-related questions in Korean."
                        full_prompt = f"**User Profile:**\n{user_profile}\n\n**User Question:**\n{user_question}"
                        
                        response = llm.create_chat_completion(messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": full_prompt}], temperature=temperature, max_tokens=max_tokens)
                        career_advice = response['choices'][0]['message']['content']
                    st.markdown("---"); st.subheader("ğŸ¤– Llama 4 Scout ì»¤ë¦¬ì–´ ì¡°ì–¸"); st.markdown(career_advice)


# 8. í‘¸í„°
st.markdown("---")
st.markdown('<div style="text-align: center; color: #666;"><p>ğŸ§  Job-Fit Insight Dashboard | Powered by Streamlit & Llama 4</p></div>', unsafe_allow_html=True)
